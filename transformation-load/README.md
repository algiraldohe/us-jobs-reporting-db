# Transformation Load Container
Service designed to retrieve a file containing the response of a job search using USA Jobs API, fetch the data within and save it into a database.

## Table of content

| Content | Description |
| :------ | :---------- |
| [1. Installation](#1-installation) | Steps for dependencies installation |
| [2. Guidelines](#2-guidelines) | Code structure and layout |
| [3. APP Usage](#3-app-usage) | How to use the `transformation-load-service` |
| [4. Future Improvements](#4-future-improvements) | Aspects that require further attention|

## 1. Installation

### 1.1. Dependencies installation

To install this project, it is recommended to use `virtualenv` python package. By default, macOS has installed the `python3` binary. So the following commands should work:


Install virtualenv to create an environment for your python packages:
```
python3 -m pip install virtualenv
```

If pip is not installed follow the guide at https://pip.pypa.io/en/stable/installation/. Use get-pip.py code.

Create an environment:
```
python3 -m virtualenv -p python3 venv
```

Activate the environment:
```
source venv/bin/activate
```

Install python dependencies:

```
pip install -r dev-requirements.txt -r requirements.txt
```

Check linting (linting configuration is set in .flake8):

```
flake8
```

Check more linting:

```
black -l 88 --preview .
```

```
isort --profile=black .
```

Check docstyle in the code (docstyle configuration is set in .pydocstyle):

```
pydocstyle
```

## 2. Guidelines

### 2.1. Files Structure

The main idea was to split the code into three cores: application, domain and infrastructure (see diagram).

![ProjectStructure](https://github.com/algiraldohe/us-jobs-reporting-db/blob/development/docs/images/ProjectStructure-TL.png)

All code relevant to how a user access data (HTTP requests, Console) should be in the application folder, that is why all frameworks code is located in this folder.

Inside `application` folder, there is a folder called `adapters`, inside, contains functions or classes that allow us to transform the requests from the user into actions that will be carried by the `domain` or business logic. This favours the possibility of including easily new adapters for the user to interact with the app, for example in a cloud implementation the request should be translated from an HTTP request, but the logic should remain doing exactly the same.

In `domain` folder is located all the code related to the business logic, it does not care about what is the body of the request containing, or what is the database we are querying, or where are we storing the data. This code uses the adapters generated by the `infrastructure` folder to get and perform what it needs in a decoupled way.

In `infrastructure` folder we have all the code related to persistence (databases, file storage), notifications (sms, mails), local services (file navigation, local storage) and external sources (Google APIs, USA Jobs API). The idea is to have adapters (classes) that handle all these sources' logic so that `domain` code can change of adapter easily and keep working.

### 2.2. Database Structure

**Database Engine:** postgres:14.1-alpine (Docker Image)
**Logical Relations**

	Table = jobs
	Fields =(
		id = Column(Integer, primary_key=True)
		PositionTitle = Column(String)
		PositionURI = Column(String)
		PositionLocation = Column(JSON)
		PositionRemuneration = Column(JSON)
		RemoteIndicator = Column(Boolean)
		StorageFile = Column(String)
		DateAdded = Column(DateTime)
		UserAdded = Column(String)
	)


Each record represents a job post with an:
`id`: Unique identifier of the job post.
`PositionTitle`: String describing the Job Post
`PositionURI`: Unique resource id
`PositionLocation`: Containing an array of all the locations where the position is available
`RemoteIndicator`: Indicates if the job is available for working remotely, is a boolean field
`StorageFile`: Specifying the path of the file from where the records are coming
`DateAdded`: Timestamp at the moment of the insertion
`UserAdded`: User of the application that added the record

***Note: At this moment no ERD is created and is preferred to keep the data in one table for seamless further extraction and manipulation. Since at the moment of the exercise no downstream analytics processes were conveyed, the decision was to make a simple loading process without many transformations and pour more focus on the service architecture to support change requirements in a more resilient and consistent manner, so the data can be easily available for different processes once given further in time.***

#### 2.2.1 Object Relational Mapping
***SQLAlchemy + Alembic***

**SQLAlchemy:** SQLAlchemy is a Python library that provides a high-level, object-oriented interface for working with relational databases. It allows you to interact with databases using Python classes and methods, abstracting the underlying SQL queries. SQLAlchemy supports various database backends and provides tools for querying, managing connections, and handling transactions.

**Alembic ORM:** Alembic is a lightweight database migration tool that works seamlessly with SQLAlchemy. It helps manage changes to your database schema over time. With Alembic, you can define and execute database migrations to create, modify, or delete database objects, such as tables and columns, while keeping your data intact.

- **Abstraction:** SQLAlchemy abstracts the SQL queries, allowing you to focus on Python code and object-oriented design.
- **Flexibility:** It supports multiple database backends, making it adaptable to your choice of database.
- **Maintainable Schema:** Alembic helps you manage schema changes gracefully, ensuring that your database evolves alongside your application.

**Usage:**
1. Define your SQLAlchemy models.
2. Generate an initial Alembic migration environment: `alembic init alembic`
3. Define your migration scripts `alembic revision --autogenerate -m "[MESSAGE]"`.
4. Apply migrations: `alembic upgrade head`
5. Use SQLAlchemy to interact with your database using Pythonic syntax.
## 3. APP Usage

### 3.1. Components

`application.adapters.console_app.py` : The console adapter takes in the requested parameters (input through the console in a local env) to translate the requested action for `domain`.

`domain.save_jobs.py` - `domain.transform_jobs`: Receives the request and uses services in `infrastructure` to perform the programmed business logic.

`infrastructure.us_jobs_service` : Handles the request and performs the HTTP request to the API retrieving the data for `domain`.

`infrastructure.database_storage_service.py` : Receives the data from `domain` and saves the data in the file storage set up in the `save_data()` method of the class.

`infrastructure.json_search_service.py` :  Receives data to look into a JSON file form `domain` and returns the necessary logic to retrieve the data from the input fields.

`migrations/`: Folder to store scripts or files that define changes to the database schema over time. These scripts are used to manage and apply changes to the structure of the database without losing data.

`postgres-data/:` This folder serves the purpose of storing the data transformed in a resilient way independently from the container volume.

Furthermore, for the correct functioning of the application I have laid down the following:

- `Makefile` is used alias the command's instructions to manipulate, set up and run the service.
- `Dockerfile` with the container configuration and required images and parameters.
- Env files `.env` and `.env.docker` with environment variables used in the extraction process one for running the service locally and the other to run it with docker.
- Template file `.env.template` to have clear guidance of the required env variables.
  
    ***Note: (Keep the env variable FILESTORAGE=../file-storage/)***

### 3.2. Commands 

#### 3.1. Docker
```bash
# Assuming you are just outside of the cloned repo
cd us-jobs-reporting-db

# 1. change the directory to the `transform-load` service directory
cd transform-load

# 2. build the image of the container using the Makefile
make build

# 3. make sure your environment is clean 

# before running this command make sure you stopped any db container 
# instances that were running
make restart-db
make clear-db # [CAREFUL!] This command will delete completely your db

# 4. create the network to communicate the containers
make create-network

# 5. create and run the database
make run-db

# (Optional) If you haven't already, create a migration of the models.py in case
# the migrations/versions/ folder is empty.
make create-migrations MESSAGE = "[MESSAGE]"

# 6. Run the migrations so the model is created in the running db
make run-migrations

# 7. run the service to extract and save the data from file into the database
make run OPERATION="save_jobs" 

# 8. run the admin panel to visualise the data (localhost:5050)
make run-admin

```
#### 3.2. Local

```bash
# Assuming you are just outside of the cloned repo
cd us-jobs-reporting-db

# 1. change the directory to the `extraction` service director
cd transform-load

# 2. activate the venv created before
source venv/bin/activate

# 3. make sure your environment is clean 

# before running this command make sure you stopped any db container 
# instances that were running
make restart-db
make clear-db # [CAREFUL!] This command will delete completely your db

# 4. create the network to communicate the containers
make create-network

# 5. create and run the database
make run-db

# (Optional) If you haven't already, create a migration of the models.py in case
# the migrations/versions/ folder is empty.
alembic revision --autogenerate -m "[MESSAGE]"

# 6. Run the migrations so the model is created in the running db
alembic upgrade head

# 7. run the service to extract and save the data from file into the database
python3 main.py --console "save_jobs"

```

After the execution of the above commands, you can check with the admin panel that your data was created. Mind your credentials in the `.env` file (`PGADMIN_DEFAULT_EMAIL`,`PGADMIN_DEFAULT_PASSWORD`)

***Remember that the admin is running in the port 5050 of the localhost.***
### 3.3. Important Functionality Considerations

##### 3.3.1. Codebase

- `../file-storage` requires permissions in docker to create the folder. You need to add the path to the folder to the following file: 
`~/Library/Group Containers/group.com.docker/settings.json` or from the docker GUI.

```json
"filesharingDirectories" : [
    "/Users",
    "/Volumes",
    "/private",
    "/tmp", 
	"[PATH_TO_SHARED_FOLDER]"
  ]
```
[See GitHub issue](https://github.com/docker/for-mac/issues/2214)

- Depending on the system `/extraction/Makefile` requires changing `$(PWD)` variable for the lower-case variation `$(pwd)`. 

- Using the `sys.argv` in Python  
  
```python
# when using the following method
application.adapters.console_app.py
	store_jobs_adapter()

# know this:

# 1
"""
self.arguments[1] correspond to the OPERATION argument
"""

# 2
extracted_data = list(map(lambda x: extract_fields_data(data=jobs_data, path=x), self.paths.values()))

# The output data structure looks something like this:
[{"Field"}:[record1, record1, record3, ... , record(n)]]

# where `record(n)` can also be another array like in the case of PositionLocation, PositionRemuneration. This type of field requires extra processing to present the data properly.

```

### 4. Future Improvements

- Map the transactions with the `MatchedObjectId`. If the system where this application will fall within or downstream services require more data and a more ACID approach, this field would help with the duplication data issue, giving the required granularity and consistency for an OLTP system.
  
- Dig deeper into the downstream requirements to optimise the storage solution depending on how that data will be accessed, and the purpose since this changes greatly the fields that need to be extracted and how and where to store the data.
  
- Security should be a main concern although the environment is local, in case of taken to production in an on-premise database solution, the developed application need to put in place controls on to who and what can access.
  
- The database operations require a status update in terms of communicating :
		- Operation carried on the database.
		- Resources impacted.
		- Status of the operation.
	

This is very necessary to handle in a more robust way the CRUD operations of the application and operationalise the functioning of the same.

- Create validation in the console adapter to be able to catch typos or map intended parameters, guiding the user in mistakes that might be made.
  
- Implement blueprints for the application to create adapters of the cloud implementation that accelerate development and delivery.

- Implement validation check points to ensure all the data that was in the file was inserted into the database. 
  
- Establish and document clearly the data policies used for INSERT operations. (`upsert, replace, append, etc...`) thinking on the business requirements.

- More robust exception and errors handling throughout the process to ensure proper feedback to the user and guarantee that any bugs that the application may present are caught properly. Specially in critic points like the data operations, since the context manager helps but is not the most reliable or extensible solution.
  
- Due to time constraints proper testing wasn't set up for the different components of the service. Although the loosely decoupled design of the application allows to implement and test components easily. Proper automated unit testing needs to be set up to contribute to the DevOps of the systems and ensure the quality of the service before deployment.